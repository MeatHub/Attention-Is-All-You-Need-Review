# Attention-Is-All-You-Need-Review
팀 MeatHub의 Attention Is All You Need 논문에 대한 분석 및 구현 과제

🚀 Attention Is All You Need 논문 분석 및 감성 분류 구현팀 MeatHub의 Transformer 아키텍처 심층 분석 및 PyTorch 기반 구현 프로젝트입니다.1. 이론 분석 (Theoretical Review)🔍 Self-Attention 메커니즘Transformer의 핵심은 입력 문장 내 모든 단어 간의 관계를 한 번에 계산하는 Self-Attention입니다.Q, K, V 생성: 입력 벡터 $X$에 선형 변환 가중치($W_Q, W_K, W_V$)를 곱해 Query, Key, Value를 생성합니다.유사도 계산: $Q$와 $K$의 내적(Dot-product)을 통해 단어 간 연관도를 계산합니다.스케일링: 내적 값이 커짐에 따라 Gradient가 작아지는 현상을 방지하기 위해 $\sqrt{d_k}$로 나누어 정규화합니다.Attention Weights: Softmax를 거쳐 각 단어가 다른 단어에 집중할 확률 분포를 만듭니다.가중합: 최종적으로 Value에 가중치를 곱해 문맥 정보가 담긴 출력을 생성합니다.🏗️ Encoder 아키텍처 흐름전체 인코더는 다음과 같은 흐름으로 $N$번(논문 기준 6층) 반복됩니다.Positional Encoding: 순환 구조(RNN)가 없는 Transformer를 위해 단어의 위치 정보를 추가합니다.Multi-Head Attention: 여러 개의 헤드가 각기 다른 관점에서 정보를 수집합니다.Feed Forward Network: 각 위치에서 독립적으로 정보를 가공합니다.Add & Norm: Residual Connection과 Layer Normalization을 통해 안정적인 학습을 지원합니다.2. PyTorch 구현 (Implementation)Hugging Face의 nn.Transformer를 사용하지 않고, PyTorch의 기초 모듈만을 사용하여 핵심 기능을 직접 구현했습니다.🛠️ 주요 구현 모듈PositionalEncoding: 사인/코사인 주기 함수 기반 위치 임베딩MultiHeadAttention: Q, K, V 분리 및 병렬 어텐션 연산TransformerSentimentModel: 인코더 블록과 분류기(Classifier) 통합3. 미니 실험: IMDB 감성 분류 (Sentiment Analysis)구현한 모델을 사용하여 영화 리뷰의 긍정/부정을 분류하는 실험을 진행했습니다.📊 실험 설정 (Hyperparameters)Dataset: IMDB Movie Reviews (25,000 train / 25,000 test)Tokenizer: bert-base-uncasedd_model: 128num_heads: 8Epochs: 10Learning Rate: 1e-4📈 결과 및 해석학습 결과: 초기 0.69 수준이었던 Loss가 10 Epoch 학습 후 0.44까지 감소하며 수렴했습니다.해석:RNN과 달리 모든 단어를 동시에 참조하므로, 문맥 파악 능력이 뛰어나 감성 키워드를 정확히 포착했습니다.병렬 연산 구조 덕분에 대량의 텍스트 데이터를 효율적으로 처리할 수 있었습니다.4. 트랜스포머의 진화와 확장BERT: Encoder를 활용한 양방향 문맥 이해GPT: Decoder를 활용한 자연스러운 텍스트 생성ViT (Vision Transformer): 이미지 패치를 단어처럼 처리하여 비전 분야로 확장
