{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeatHub/Attention-Is-All-You-Need-Review/blob/main/transformer_%EA%B5%AC%ED%98%842.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets transformers"
      ],
      "metadata": {
        "id": "PT5tCr_whz8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from dataclasses import dataclass\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "YYAsvzSvhx_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('imdb')\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "Vk519X5OhzlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "yxNKTE3gkYkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3GooWp4heVH"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=256)\n",
        "\n",
        "train_data = dataset['train'].shuffle(seed=42)\n",
        "test_data = dataset['test'].shuffle(seed=42)\n",
        "\n",
        "train_tokenized = train_data.map(tokenize, batched=True)\n",
        "test_tokenized = test_data.map(tokenize, batched=True)\n",
        "\n",
        "def make_loader(data, batch_size=16):\n",
        "    ids = torch.tensor(data['input_ids'])\n",
        "    labels = torch.tensor(data['label'])\n",
        "    return DataLoader(TensorDataset(ids, labels), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_loader = make_loader(train_tokenized, batch_size=16)\n",
        "test_loader = make_loader(test_tokenized, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        # div_term = 1 / 10000^(2i/d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x + PE: Embedding Î≤°ÌÑ∞Ïóê ÏúÑÏπò Ï†ïÎ≥¥ Ìï©ÏÇ∞\n",
        "        return self.dropout(x + self.pe[:, :x.size(1)])"
      ],
      "metadata": {
        "id": "AZILqhC3h7wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Linear projection: Q, K, V ÏÉùÏÑ± (Query, Key, Value)\n",
        "        # Split into heads: (batch, seq_len, num_heads, d_k) -> (batch, num_heads, seq_len, d_k)\n",
        "        q = self.w_q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.w_k(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.w_v(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Scaled Dot-Product Attention: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\n",
        "        context = torch.matmul(self.dropout(attn), v).transpose(1, 2).reshape(batch_size, -1, self.num_heads * self.d_k)\n",
        "        return self.fc(context)"
      ],
      "metadata": {
        "id": "BNJ6BfcNnIcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model * 4, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Sublayer 1: Residual Connection & Layer Normalization -> LayerNorm(x + Sublayer(x))\n",
        "        attn_out = self.mha(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # Sublayer 2: Position-wise Feed-Forward Network & LayerNorm\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_out))\n",
        "        return x"
      ],
      "metadata": {
        "id": "sTvsimBqlS5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerSentimentModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers=2, num_classes=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Input Embedding: Î¨∏ÏûêÎ•º d_model Ï∞®ÏõêÏùò Î≤°ÌÑ∞Î°ú Î≥ÄÌôò\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
        "\n",
        "        # N x Encoder Layers: ÎÖºÎ¨∏Ïùò Nx Íµ¨Ï°∞ (Ï§ëÏ≤©Îêú Ïù∏ÏΩîÎçî Ï∏µ)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        # Final Linear Layer: Î∂ÑÎ•òÎ•º ÏúÑÌïú Ï∂úÎ†•Ï∏µ\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Global Average Pooling: (batch, seq_len, d_model) -> (batch, d_model)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "DZyQBzLalS0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, config, train_loader, test_loader):\n",
        "        self.model = model.to(config.device)\n",
        "        self.config = config\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.lr)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.config.epochs):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            for batch in tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "                ids, labels = [b.to(self.config.device) for b in batch]\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = self.criterion(self.model(ids), labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            acc = self.evaluate()\n",
        "            print(f\"Loss: {total_loss/len(self.train_loader):.4f} | Test Acc: {acc:.2f}%\")\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in self.test_loader:\n",
        "                ids, labels = [b.to(self.config.device) for b in batch]\n",
        "                out = self.model(ids)\n",
        "                correct += (out.argmax(1) == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        return 100 * correct / total\n",
        "\n",
        "def predict_sentiment(text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding='max_length',\n",
        "                          truncation=True, max_length=256).to(device)\n",
        "        output = model(inputs['input_ids'])\n",
        "        prob = torch.softmax(output, dim=-1)\n",
        "        pred = output.argmax(1).item()\n",
        "        label = \"Í∏çÏ†ï üòä\" if pred == 1 else \"Î∂ÄÏ†ï üò°\"\n",
        "        print(f\"Î¶¨Î∑∞: {text}\\nÍ≤∞Í≥º: {label} ({prob[0][pred].item()*100:.2f}%)\")\n",
        "        print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "t5eYuqNrlSrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    vocab_size: int = tokenizer.vocab_size\n",
        "    d_model: int = 128\n",
        "    num_heads: int = 8\n",
        "    epochs: int = 12\n",
        "    lr: float = 1e-4\n",
        "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "config = Config()\n",
        "model = TransformerSentimentModel(config.vocab_size, config.d_model, config.num_heads)"
      ],
      "metadata": {
        "id": "s3VLuZualSpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model, config, train_loader, test_loader)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "fbDOnKSPiX0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = [\n",
        "    \"This movie was a masterpiece. The depth of the characters was incredible.\",\n",
        "    \"I hated this film. It was way too long and very boring.\",\n",
        "    \"It was okay, but the ending was a bit disappointing.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    predict_sentiment(text, model, tokenizer, config.device)"
      ],
      "metadata": {
        "id": "Vw0cm_nVlSnc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}